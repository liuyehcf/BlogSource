---
title: Kubernetes-阿里云公开课学习笔记
date: 2019-11-20 09:17:34
tags: 
- 摘录
categories: 
- Kubernetes
---

__阅读更多__

<!--more-->

# 1 云原生概述

## 1.1 云原生技术发展简史

首先从第一个问题进行分享，那就是“为什么要开设云原生技术公开课？”云原生、`CNCF`都是目前非常热门的关键词，但是这些技术并不是非常新鲜的内容。

1. 2004年—2007年，`Google`已在内部大规模地使用像`Cgroups`这样的容器技术
1. 2008年，`Google`将`Cgroups`合并进入了Linux内核主干
1. 2013年，`Docker`项目正式发布
1. 2014年，`Kubernetes`项目也正式发布。这样的原因也非常容易理解，因为有了容器和`Docker`之后，就需要有一种方式去帮助大家方便、快速、优雅地管理这些容器，这就是 `Kubernetes`项目的初衷。在`Google`和`Redhat`发布了`Kubernetes`之后，这个项目的发展速度非常之快
1. 2015年，由`Google`、`Redhat`以及微软等大型云计算厂商以及一些开源公司共同牵头成立了`CNCF`云原生基金会。`CNCF`成立之初，就有22个创始会员，而且`Kubernetes`也成为了`CNCF`托管的第一个开源项目。在这之后，`CNCF`的发展速度非常迅猛
1. 2017年，`CNCF`达到170个成员和14个基金项目
1. 2018年，`CNCF`成立三周年有了195个成员，19个基金会项目和11个孵化项目，如此之快的发展速度在整个云计算领域都是非常罕见的

## 1.2 我们正处于时代的关键节点

2019年正是云原生时代的关键节点，为什么这么说？我们这里就为大家简单梳理一下

从2013年`Docker`项目发布开始说起，`Docker`项目的发布使得全操作系统语义的沙盒技术唾手可得，使得用户能够更好地、更完整地打包自己的应用，使得开发者可以轻而易举的获得了一个应用的最小可运行单位，而不需要依赖任何`PaaS`能力。这对经典PaaS产业其实是一个“降维打击”

2014年的时候，`Kubernetes`项目发布，其意义在于`Google`将内部的`Borg/Omega`系统思想借助开源社区实现了“重生”，并且提出了“容器设计模式”的思想。而`Google`之所以选择间接开源`Kubernetes`而不是直接开源`Borg`项目，其实背后的原因也比较容易理解：`Borg/Omega`这样的系统太复杂了，是没办法提供给`Google`之外的人使用，但是`Borg/Omega`这样的设计思想却可以借助`Kubernetes`让大家接触到，这也是开源`Kubernetes`的重要背景

这样到了2015年-2016年，就到了容器编排“三国争霸”的时代，当时`Docker`、`Swarm`、`Mesos`、`Kubernetes`都在容器编排领域展开角逐，他们竞争的原因其实也比较容易理解， 那就是`Docker`或者容器本身的价值虽然大，但是如果想要让其产生商业价值或者说对云的价值，那么就一定需要在编排上面占据一个有利的位置

`Swarm`和`Mesos`的特点，那就是各自只在生态和技术方面比较强，其中，`Swarm`更偏向于生态，而`Mesos`技术更强一些。相比之下，`Kubernetes`则兼具了两者优势，最终在 2017年“三国争霸”的局面中得以胜出，成为了当时直到现在的容器编排标准。这一过程的代表性事件就是`Docker`公司宣布在核心产品中内置了`Kubernetes`服务，并且`Swarm`项目逐渐停止维护

到了2018年的时候，云原生技术理念开始逐渐萌芽，这是因为此时`Kubernetes`以及容器都成为了云厂商的既定标准，以“云”为核心的软件研发思想逐步形成

而到了2019年，情况似乎又将发生一些变化

## 1.3 2019年——云原生技术普及元年

为什么说2019年很可能是一个关键节点呢？我们认为2019年是云原生技术的普及元年

首先大家可以看到，在2019年，阿里巴巴宣布要全面上云，而且“上云就要上云原生”。我们还可以看到，以“云”为核心的软件研发思想，正逐步成为所有开发者的默认选项。像`Kubernetes`等云原生技术正在成为技术人员的必修课，大量的工作岗位正在涌现出来

这种背景下，“会`Kubernetes`”已经远远不够了，“懂`Kubernetes`”、“会云原生架构”的重要性正日益凸显出来。从2019年开始，云原生技术将会大规模普及，这也是为什么大家都要在这个时间点上学习和投资云原生技术的重要原因

## 1.4 云原生的技术范畴

云原生的技术范畴包括了以下几个方面：

1. 第一部分是云应用定义与开发流程。这包括应用定义与镜像制作、配置`CI/CD`、消息和`Streaming`以及数据库等
1. 第二部分是云应用的编排与管理流程。这也是`Kubernetes`比较关注的一部分，包括了应用编排与调度、服务发现治理、远程调用、API 网关以及`Service Mesh`
1. 第三部分是监控与可观测性。这部分所强调的是云上应用如何进行监控、日志收集、Tracing以及在云上如何实现破坏性测试，也就是混沌工程的概念
1. 第四部分就是云原生的底层技术，比如容器运行时、云原生存储技术、云原生网络技术等
1. 第五部分是云原生工具集，在前面的这些核心技术点之上，还有很多配套的生态或者周边的工具需要使用，比如流程自动化与配置管理、容器镜像仓库、云原生安全技术以及云端密码管理等
1. 最后则是`Serverless`。`Serverless`是一种`PaaS`的特殊形态，它定义了一种更为“极端抽象”的应用编写方式，包含了`FaaS`和`BaaS`这样的概念。而无论是`FaaS`还是`BaaS`，其最为典型的特点就是按实际使用计费（Pay as you go），因此`Serverless`计费也是重要的知识和概念

## 1.5 云原生思想的两个理论

在了解完云原生的技术范畴之后你就会发现，其所包含的技术内容还是很多的，但是这些内容的技术本质却是类似的。云原生技术的本质是两个理论基础。

1. __第一个理论基础是：不可变基础设施__。这一点目前是通过容器镜像来实现的，其含义就是应用的基础设施应该是不可变的，是一个自包含、自描述可以完全在不同环境中迁移的东西
1. __第二个理论基础就是：云应用编排理论__。当前的实现方式就是`Google`所提出来的“容器设计模式”，这也是本系列课程中的`Kubernetes`部分所需主要讲解的内容

## 1.6 基础设施向云演进的过程

首先为大家介绍一下“不可变基础设施”的概念。其实，应用所依赖的基础设施也在经历一个向云演进的过程，举例而言，对于传统的应用基础设施而言，其实往往是可变的。

大家可能经常会干这样一件事情，比如需要发布或者更新一个软件，那么流程大致是这样的，先通过`SSH`连到服务器，然后手动升级或者降级软件包，逐个调整服务器上的配置文件，并且将新代码直接都部署到现有服务器上。因此，这套基础设施会不断地被调整和修改

但是在云上，对“云”友好的应用基础设施是不可变的

这种场景下的上述更新过程会这么做：一旦应用部署完成之后，那么这套应用基础设施就不会再修改了。如果需要更新，那么需要现更改公共镜像来构建新服务直接替换旧服务。而我们之所以能够实现直接替换，就是因为容器提供了自包含的环境（包含应用运行所需的所有依赖）。所以对于应用而言，完全不需要关心容器发生了什么变化，只需要把容器镜像本身修改掉就可以了。因此，对于云友好的基础设施是随时可以替换和更换的，这就是因为容器具有敏捷和一致性的能力，也就是云时代的应用基础设施

所以，总结而言，云时代的基础设施就像是可以替代的“牲口”，可以随时替换；而传统的基础设施则是独一无二的“宠物”，需要细心呵护，这就体现出了云时代不可变基础设施的优点

## 1.7 基础设施向云演进的意义

所以，像这样的基础设施向“不可变”演进的过程，为我们提供了两个非常重要的优点

1. 基础设施的一致性和可靠性。同样一个镜像，无论是在美国打开，在中国打开，还是在印度打开都是一样的。并且其中的`OS`环境对于应用而言都是一致的。而对于应用而言，它就不需要关心容器跑在哪里，这就是基础设施一致性非常重要的一个特征
1. 这样的镜像本身就是自包含的，其包含了应用运行所需要的所有依赖，因此也可以漂移到云上的任何一个位置

此外，云原生的基础设施还提供了简单、可预测的部署和运维能力。由于现在有了镜像，应用还是自描述的，通过镜像运行起来的整个容器其实可以像`Kubernetes`的`Operator`技术一样将其做成自运维的，所以整个应用本身都是自包含的行为，使得其能够迁移到云上任何一个位置。这也使得整个流程的自动化变得非常容易

应用本身也可以更好地扩容，从1个实例变成100个实例，进而变成1万个实例，这个过程对于容器化后的应用没有任何特殊的。最后，我们这时也能够通过不可变的基础设施来地快速周围的管控系统和支撑组件。因为，这些组件本身也是容器化的，是符合不可变基础设施这样一套理论的组件

以上就是不可变基础设施为用户带来的最大的优点

## 1.8 云原生关键技术点

当我们回过头来看云原生关键技术点或者说它所依赖的技术理论的时候，可以看到主要有这样的四个方向：

1. 如何构建自包含、可定制的应用镜像
1. 能不能实现应用快速部署与隔离能力
1. 应用基础设施创建和销毁的自动化管理
1. 可复制的管控系统和支撑组件

# 2 容器基本概念

## 2.1 容器与镜像

__什么是容器？__

在介绍容器的具体概念之前，先简单回顾一下操作系统是如何管理进程的

首先，当我们登录到操作系统之后，可以通过`ps`等操作看到各式各样的进程，这些进程包括系统自带的服务和用户的应用进程。那么，这些进程都有什么样的特点？

1. 这些进程可以相互看到、相互通信
1. 它们使用的是同一个文件系统，可以对同一个文件进行读写操作
1. 这些进程会使用相同的系统资源

__这样的三个特点会带来什么问题呢？__

1. 因为这些进程能够相互看到并且进行通信，高级权限的进程可以攻击其他进程
1. 因为它们使用的是同一个文件系统，因此会带来两个问题
    * 这些进程可以对于已有的数据进行增删改查，具有高级权限的进程可能会将其他进程的数据删除掉，破坏掉其他进程的正常运行
    * 此外，进程与进程之间的依赖可能会存在冲突，如此一来就会给运维带来很大的压力
1. 因为这些进程使用的是同一个宿主机的资源，应用之间可能会存在资源抢占的问题，当一个应用需要消耗大量`CPU`和内存资源的时候，就可能会破坏其他应用的运行，导致其他应用无法正常地提供服务

__针对上述的三个问题，如何为进程提供一个独立的运行环境呢？__

针对不同进程使用同一个文件系统所造成的问题而言，`Linux`和`Unix`操作系统可以通过`chroot`系统调用将子目录变成根目录，达到视图级别的隔离；进程在`chroot`的帮助下可以具有独立的文件系统，对于这样的文件系统进行增删改查不会影响到其他进程

因为进程之间相互可见并且可以相互通信，使用`Namespace`技术来实现进程在资源的视图上进行隔离。在`chroot`和`Namespace`的帮助下，进程就能够运行在一个独立的环境下了

但在独立的环境下，进程所使用的还是同一个操作系统的资源，一些进程可能会侵蚀掉整个系统的资源。为了减少进程彼此之间的影响，可以通过`Cgroup`来限制其资源使用率，设置其能够使用的`CPU`以及内存量

__那么，应该如何定义这样的进程集合呢？其实，容器就是一个视图隔离、资源可限制、独立文件系统的进程集合__

* 所谓“视图隔离”就是能够看到部分进程以及具有独立的主机名等
* 控制资源使用率则是可以对于内存大小以及`CPU`使用个数等进行限制
* 容器就是一个进程集合，它将系统的其他资源隔离开来，具有自己独立的资源视图
* 容器具有一个独立的文件系统，因为使用的是系统的资源，所以在独立的文件系统内不需要具备内核相关的代码或者工具，我们只需要提供容器所需的二进制文件、配置文件以及依赖即可。只要容器运行时所需的文件集合都能够具备，那么这个容器就能够运行起来

__什么是镜像？综上所述，我们将这些容器运行时所需要的所有的文件集合称之为容器镜像__

那么，一般都是通过什么样的方式来构建镜像的呢？通常情况下，我们会采用`Dockerfile`来构建镜像，这是因为`Dockerfile`提供了非常便利的语法糖，能够帮助我们很好地描述构建的每个步骤。当然，每个构建步骤都会对已有的文件系统进行操作，这样就会带来文件系统内容的变化，我们将这些变化称之为`changeset`。当我们把构建步骤所产生的变化依次作用到一个空文件夹上，就能够得到一个完整的镜像

`changeset`的分层以及复用特点能够带来几点优势：

1. 能够提高分发效率，简单试想一下，对于大的镜像而言，如果将其拆分成各个小块就能够提高镜像的分发效率，这是因为镜像拆分之后就可以并行下载这些数据
1. 因为这些数据是相互共享的，也就意味着当本地存储上包含了一些数据的时候，只需要下载本地没有的数据即可，举个简单的例子就是`golang`镜像是基于`alpine`镜像进行构建的，当本地已经具有了`alpine`镜像之后，在下载`golang`镜像的时候只需要下载本地`alpine`镜像中没有的部分即可
1. 因为镜像数据是共享的，因此可以节约大量的磁盘空间，简单设想一下，当本地存储具有了`alpine`镜像和`golang`镜像，在没有复用的能力之前，`alpine`镜像具有`5M`大小，`golang`镜像有`300M`大小，因此就会占用`305M`空间；而当具有了复用能力之后，只需要`300M`空间即可

__如何构建镜像？__

1. `FROM`行表示以下的构建步骤基于什么镜像进行构建，正如前面所提到的，镜像是可以复用的
1. `WORKDIR`行表示会把接下来的构建步骤都在哪一个相应的具体目录下进行，其起到的作用类似于`Shell`里面的`cd`
1. `COPY`行表示的是可以将宿主机上的文件拷贝到容器镜像内
1. `RUN`行表示在具体的文件系统内执行相应的动作。当我们运行完毕之后就可以得到一个应用了
1. `CMD`行表示使用镜像时的默认程序名字

当有了`Dockerfile`之后，就可以通过`docker build`命令构建出所需要的应用。构建出的结果存储在本地，一般情况下，镜像构建会在打包机或者其他的隔离环境下完成

那么，这些镜像如何运行在生产环境或者测试环境上呢？这时候就需要一个中转站或者中心存储，我们称之为`docker registry`，也就是镜像仓库，其负责存储所有产生的镜像数据。我们只需要通过`docker push`就能够将本地镜像推动到镜像仓库中，这样一来，就能够在生产环境上或者测试环境上将相应的数据下载下来并运行了

__如何运行容器？运行一个容器一般情况下分为三步：__

1. 从镜像仓库中将相应的镜像下载下来
1. 当镜像下载完成之后就可以通过`docker images`来查看本地镜像，这里会给出一个完整的列表，我们可以在列表中选中想要的镜像
1. 当选中镜像之后，就可以通过`docker run`来运行这个镜像得到想要的容器，当然可以通过多次运行得到多个容器。一个镜像就相当于是一个模板，一个容器就像是一个具体的运行实例，因此镜像就具有了一次构建、到处运行的特点

__小结__

简单回顾一下，容器就是和系统其它部分隔离开来的进程集合，这里的其他部分包括进程、网络资源以及文件系统等。而镜像就是容器所需要的所有文件集合，其具备一次构建、到处运行的特点

## 2.2 容器的生命周期

__容器运行时的生命周期__

容器是一组具有隔离特性的进程集合，在使用`docker run`的时候会选择一个镜像来提供独立的文件系统并指定相应的运行程序。这里指定的运行程序称之为`initial`进程，这个`initial`进程启动的时候，容器也会随之启动，当`initial`进程退出的时候，容器也会随之退出。

因此，可以认为容器的生命周期和`initial`进程的生命周期是一致的。当然，因为容器内不只有这样的一个`initial`进程，`initial`进程本身也可以产生其他的子进程或者通过`docker exec`产生出来的运维操作，也属于`initial`进程管理的范围内。当`initial`进程退出的时候，所有的子进程也会随之退出，这样也是为了防止资源的泄漏

但是这样的做法也会存在一些问题，首先应用里面的程序往往是有状态的，其可能会产生一些重要的数据，当一个容器退出被删除之后，数据也就会丢失了，这对于应用方而言是不能接受的，所以需要将容器所产生出来的重要数据持久化下来。容器能够直接将数据持久化到指定的目录上，这个目录就称之为数据卷

数据卷有一些特点，其中非常明显的就是数据卷的生命周期是独立于容器的生命周期的，也就是说容器的创建、运行、停止、删除等操作都和数据卷没有任何关系，因为它是一个特殊的目录，是用于帮助容器进行持久化的。简单而言，我们会将数据卷挂载到容器内，这样一来容器就能够将数据写入到相应的目录里面了，而且容器的退出并不会导致数据的丢失

__通常情况下，数据卷管理主要有两种方式：__

1. 通过`bind`的方式，直接将宿主机的目录直接挂载到容器内。这种方式比较简单，但是会带来运维成本，因为其依赖于宿主机的目录，需要对于所有的宿主机进行统一管理
1. 将目录管理交给运行引擎

## 2.3 容器项目架构

__`moby`容器引擎架构__

`moby`是目前最流行的容器管理引擎，`moby daemon`会对上提供有关于容器、镜像、网络以及 `Volume`的管理。`moby daemon`所依赖的最重要的组件就是`containerd`，`containerd`是一个容器运行时管理引擎，其独立于`moby daemon`，可以对上提供容器、镜像的相关管理

`containerd`底层有`containerd shim`模块，其类似于一个守护进程，这样设计的原因有几点：

1. 首先，`containerd`需要管理容器生命周期，而容器可能是由不同的容器运行时所创建出来的，因此需要提供一个灵活的插件化管理。而`shim`就是针对于不同的容器运行时所开发的，这样就能够从`containerd`中脱离出来，通过插件的形式进行管理
1. 其次，因为`shim`插件化的实现，使其能够被`containerd`动态接管。如果不具备这样的能力，当`moby daemon`或者`containerd daemon`意外退出的时候，容器就没人管理了，那么它也会随之消失、退出，这样就会影响到应用的运行
1. 最后，因为随时可能会对`moby`或者`containerd`进行升级，如果不提供`shim`机制，那么就无法做到原地升级，也无法做到不影响业务的升级，因此`containerd shim`非常重要，它实现了动态接管的能力

## 2.4 容器 VS VM

__容器和`VM`之间的差异__

`VM`利用`Hypervisor`虚拟化技术来模拟`CPU`、内存等硬件资源，这样就可以在宿主机上建立一个 `Guest OS`，这是常说的安装一个虚拟机

每一个`Guest OS`都有一个独立的内核，比如`Ubuntu`、`CentOS`甚至是`Windows`等，在这样的 `Guest OS`之下，每个应用都是相互独立的，`VM`可以提供一个更好的隔离效果。但这样的隔离效果需要付出一定的代价，因为需要把一部分的计算资源交给虚拟化，这样就很难充分利用现有的计算资源，并且每个`Guest OS`都需要占用大量的磁盘空间，比如`Windows`操作系统的安装需要`10~30G`的磁盘空间，`Ubuntu`也需要 5~6G，同时这样的方式启动很慢。正是因为虚拟机技术的缺点，催生出了容器技术

容器是针对于进程而言的，因此无需`Guest OS`，只需要一个独立的文件系统提供其所需要文件集合即可。所有的文件隔离都是进程级别的，因此启动时间快于`VM`，并且所需的磁盘空间也小于`VM`。当然了，进程级别的隔离并没有想象中的那么好，隔离效果相比`VM`要差很多。

总体而言，容器和`VM`相比，各有优劣，因此容器技术也在向着强隔离方向发展

## 2.5 总结

1. 容器是一个进程集合，具有自己独特的视图视角
1. 镜像是容器所需要的所有文件集合，其具备一次构建、到处运行的特点
1. 容器的生命周期和`initial`进程的生命周期是一样的
1. 容器和`VM`相比，各有优劣，容器技术在向着强隔离方向发展

# 3 Kubernetes核心概念

## 3.1 什么是Kubernetes

`Kubernetes`，从官方网站上可以看到，它是一个工业级的容器编排平台。`Kubernetes`这个单词是希腊语，它的中文翻译是“舵手”或者“飞行员”。在一些常见的资料中也会看到“ks”这个词，也就是“k8s”，它是通过将8个字母“ubernete”替换为“8”而导致的一个缩写

`Kubernetes`为什么要用“舵手”来命名呢？大家可以看一下这张图：

![3-1-1](/images/Kubernetes-阿里云公开课学习笔记/3-1-1.png)

这是一艘载着一堆集装箱的轮船，轮船在大海上运着集装箱奔波，把集装箱送到它们该去的地方。我们之前其实介绍过一个概念叫做`container`，`container`这个英文单词也有另外的一个意思就是“集装箱”。`Kubernetes`也就借着这个寓意，希望成为运送集装箱的一个轮船，来帮助我们管理这些集装箱，也就是管理这些容器

这个就是为什么会选用`Kubernetes`这个词来代表这个项目的原因。更具体一点地来说：`Kubernetes`是一个自动化的容器编排平台，它负责应用的部署、应用的弹性以及应用的管理，这些都是基于容器的

## 3.2 Kubernetes的几个核心的功能

1. 服务的发现与负载的均衡
1. 容器的自动装箱，我们也会把它叫做`scheduling`，就是“调度”，把一个容器放到一个集群的某一个机器上，`Kubernetes`会帮助我们去做存储的编排，让存储的声明周期与容器的生命周期能有一个连接
1. `Kubernetes`会帮助我们去做自动化的容器的恢复。在一个集群中，经常会出现宿主机的问题或者说是`OS`的问题，导致容器本身的不可用，`Kubernetes`会自动地对这些不可用的容器进行恢复
1. `Kubernetes`会帮助我们去做应用的自动发布与应用的回滚，以及与应用相关的配置密文的管理
1. 对于`job`类型任务，`Kubernetes`可以去做批量的执行
1. 为了让这个集群、这个应用更富有弹性，`Kubernetes`也支持水平的伸缩

下面，我们希望以三个例子跟大家更切实地介绍一下`Kubernetes`的能力

### 3.2.1 调度

`Kubernetes`可以把用户提交的容器放到`Kubernetes`管理的集群的某一台节点上去。`Kubernetes`的调度器是执行这项能力的组件，它会观察正在被调度的这个容器的大小、规格

比如说它所需要的`CPU`以及它所需要的`memory`，然后在集群中找一台相对比较空闲的机器来进行一次`placement`，也就是一次放置的操作。在这个例子中，它可能会把红颜色的这个容器放置到第二个空闲的机器上，来完成一次调度的工作

![3-2-1](/images/Kubernetes-阿里云公开课学习笔记/3-2-1.png)

### 3.2.2 自动修复

`Kubernetes`有一个节点健康检查的功能，它会监测这个集群中所有的宿主机，当宿主机本身出现故障，或者软件出现故障的时候，这个节点健康检查会自动对它进行发现

下面`Kubernetes`会把运行在这些失败节点上的容器进行自动迁移，迁移到一个正在健康运行的宿主机上，来完成集群内容器的一个自动恢复

![3-2-2](/images/Kubernetes-阿里云公开课学习笔记/3-2-2.png)

### 3.2.3 水平伸缩

`Kubernetes`有业务负载检查的能力，它会监测业务上所承担的负载，如果这个业务本身的`CPU`利用率过高，或者响应时间过长，它可以对这个业务进行一次扩容

比如说在下面的例子中，黄颜色的过度忙碌，`Kubernetes`就可以把黄颜色负载从一份变为三份。接下来，它就可以通过负载均衡把原来打到第一个黄颜色上的负载平均分到三个黄颜色的负载上去，以此来提高响应的时间

![3-2-3](/images/Kubernetes-阿里云公开课学习笔记/3-2-3.png)

## 3.3 Kubernetes的架构

`Kubernetes`架构是一个比较典型的`二层架构`和`server-client`架构。`Master`作为中央的管控节点，会去与`Node`进行一个连接

所有`UI`的、`clients`、这些`user`侧的组件，只会和`Master`进行连接，把希望的状态或者想执行的命令下发给`Master`，`Master`会把这些命令或者状态下发给相应的节点，进行最终的执行

![3-3-1](/images/Kubernetes-阿里云公开课学习笔记/3-3-1.png)

`Kubernetes`的`Master`包含四个主要的组件：`API Server`、`Controller`、`Scheduler`以及`etcd`。如下图所示：

![3-3-2](/images/Kubernetes-阿里云公开课学习笔记/3-3-2.png)

* __`API Server`__：顾名思义是用来处理`API`操作的，`Kubernetes`中所有的组件都会和`API Server`进行连接，组件与组件之间一般不进行独立的连接，都依赖于`API Server`进行消息的传送
* __`Controller`__：是控制器，它用来完成对集群状态的一些管理。比如刚刚我们提到的两个例子之中，第一个自动对容器进行修复、第二个自动进行水平扩张，都是由`Kubernetes`中的`Controller`来进行完成的
* __`Scheduler`__：是调度器，“调度器”顾名思义就是完成调度的操作，就是我们刚才介绍的第一个例子中，把一个用户提交的`Container`，依据它对`CPU`、对`memory`请求大小，找一台合适的节点，进行放置
* __`etcd`__：是一个分布式的一个存储系统，`API Server`中所需要的这些原信息都被放置在`etcd`中，`etcd`本身是一个高可用系统，通过`etcd`保证整个`Kubernetes`的`Master`组件的高可用性

我们刚刚提到的`API Server`，它本身在部署结构上是一个可以水平扩展的一个部署组件；`Controller`是一个可以进行热备的一个部署组件，它只有一个`active`，它的调度器也是相应的，虽然只有一个`active`，但是可以进行热备

__`Kubernetes`的架构`Node`__

`Kubernetes`的`Node`是真正运行业务负载的，每个业务负载会以`Pod`的形式运行。等一下我会介绍一下`Pod`的概念。一个`Pod`中运行的一个或者多个容器，真正去运行这些`Pod`的组件的是叫做`kubelet`，也就是`Node`上最为关键的组件，它通过`API Server`接收到所需要`Pod`运行的状态，然后提交到我们下面画的这个`Container Runtime`组件中

![3-3-3](/images/Kubernetes-阿里云公开课学习笔记/3-3-3.png)

在`OS`上去创建容器所需要运行的环境，最终把容器或者`Pod`运行起来，也需要对存储跟网络进行管理。`Kubernetes`并不会直接进行网络存储的操作，他们会靠`Storage Plugin`或者是网络的 `Plugin`来进行操作。用户自己或者云厂商都会去写相应的`Storage Plugin`或者`Network Plugin`，去完成存储操作或网络操作

在`Kubernetes`自己的环境中，也会有`Kubernetes`的`Network`，它是为了提供`Service network`来进行搭网组网的。（等一下我们也会去介绍“service”这个概念。）真正完成`service`组网的组件的是`Kube-proxy`，它是利用了`iptable`的能力来进行组建`Kubernetes`的`Network`，就是`cluster network`，以上就是`Node`上面的四个组件

`Kubernetes`的`Node`并不会直接和`user`进行`interaction`，它的`interaction`只会通过`Master`。而`User`是通过`Master`向节点下发这些信息的。`Kubernetes`每个`Node`上，都会运行我们刚才提到的这几个组件

下面我们以一个例子再去看一下`Kubernetes`架构中的这些组件，是如何互相进行`interaction`的

![3-3-4](/images/Kubernetes-阿里云公开课学习笔记/3-3-4.png)

用户可以通过`UI`或者`CLI`提交一个`Pod`给`Kubernetes`进行部署，这个`Pod`请求首先会通过`CLI`或者`UI`提交给`Kubernetes API Server`，下一步`API Server`会把这个信息写入到它的存储系统`etcd`，之后`Scheduler`会通过`API Server`的`watch`或者叫做`notification`机制得到这个信息：有一个`Pod`需要被调度

这个时候`Scheduler`会根据它的内存状态进行一次调度决策，在完成这次调度之后，它会向`API Server report`说：“OK！这个 Pod 需要被调度到某一个节点上。”

这个时候`API Server`接收到这次操作之后，会把这次的结果再次写到`etcd`中，然后`API Server`会通知相应的节点进行这次`Pod`真正的执行启动。相应节点的`kubelet`会得到这个通知，`kubelet`就会去调`Container runtime`来真正去启动配置这个容器和这个容器的运行环境，去调度`Storage Plugin`来去配置存储，`network Plugin`去配置网络

这个例子我们可以看到：这些组件之间是如何相互沟通相互通信，协调来完成一次`Pod`的调度执行操作的

## 3.4 Kubernetes的核心概念与它的API

### 3.4.1 核心概念

#### 3.4.1.1 第一个概念：Pod

__`Pod`是`Kubernetes`的一个最小调度以及资源单元__。用户可以通过`Kubernetes`的`Pod API`生产一个`Pod`，让`Kubernetes`对这个`Pod`进行调度，也就是把它放在某一个`Kubernetes`管理的节点上运行起来。一个`Pod`简单来说是对一组容器的抽象，它里面会包含一个或多个容器

比如像下面的这幅图里面，它包含了两个容器，每个容器可以指定它所需要资源大小。比如说，`1`个核`1`个`G`，或者说`0.5`个核，`0.5`个`G`

当然在这个`Pod`中也可以包含一些其他所需要的资源：比如说我们所看到的`Volume`卷这个存储资源；比如说我们需要`100`个`GB`的存储或者`20GB`的另外一个存储

在`Pod`里面，我们也可以去定义容器所需要运行的方式。比如说运行容器的`Command`，以及运行容器的环境变量等等。`Pod`这个抽象也给这些容器提供了一个共享的运行环境，它们会共享同一个网络环境，这些容器可以用`localhost`来进行直接的连接。而`Pod`与`Pod`之间，是互相有`isolation`隔离的

![3-4-1](/images/Kubernetes-阿里云公开课学习笔记/3-4-1.png)

#### 3.4.1.2 第二个概念：Volume

`Volume`就是卷的概念，它是用来管理`Kubernetes`存储的，是用来声明在`Pod`中的容器可以访问文件目录的，一个卷可以被挂载在`Pod`中一个或者多个容器的指定路径下面

而`Volume`本身是一个抽象的概念，一个`Volume`可以去支持多种的后端的存储。比如说`Kubernetes`的`Volume`就支持了很多存储插件，它可以支持本地的存储，可以支持分布式的存储，比如说像`ceph`，`GlusterFS`；它也可以支持云存储，比如说阿里云上的云盘、`AWS`上的云盘、`Google`上的云盘等等

![3-4-2](/images/Kubernetes-阿里云公开课学习笔记/3-4-2.png)

#### 3.4.1.3 第三个概念：Deployment

`Deployment`是在`Pod`这个抽象上更为上层的一个抽象，它可以定义一组`Pod`的副本数目、以及这个`Pod`的版本。一般大家用`Deployment`这个抽象来做应用的真正的管理，而`Pod`是组成`Deployment`最小的单元

`Kubernetes`是通过`Controller`，也就是我们刚才提到的控制器去维护`Deployment`中`Pod`的数目，它也会去帮助`Deployment`自动恢复失败的`Pod`

比如说我可以定义一个`Deployment`，这个`Deployment`里面需要两个`Pod`，当一个`Pod`失败的时候，控制器就会监测到，它重新把`Deployment`中的`Pod`数目从一个恢复到两个，通过再去新生成一个`Pod`。通过控制器，我们也会帮助完成发布的策略。比如说进行滚动升级，进行重新生成的升级，或者进行版本的回滚

![3-4-3](/images/Kubernetes-阿里云公开课学习笔记/3-4-3.png)

#### 3.4.1.4 第四个概念：Service

`Service`提供了一个或者多个`Pod`实例的稳定访问地址

比如在上面的例子中，我们看到：一个`Deployment`可能有两个甚至更多个完全相同的`Pod`。对于一个外部的用户来讲，访问哪个`Pod`其实都是一样的，所以它希望做一次负载均衡，在做负载均衡的同时，我只想访问某一个固定的`VIP`，也就是`Virtual IP`地址，而不希望得知每一个具体的`Pod`的`IP`地址。

我们刚才提到，这个`Pod`本身可能`terminal go`（终止），如果一个`Pod`失败了，可能会换成另外一个新的

对一个外部用户来讲，提供了多个具体的`Pod`地址，这个用户要不停地去更新`Pod`地址，当这个`Pod`再失败重启之后，我们希望有一个抽象，把所有`Pod`的访问能力抽象成一个第三方的一个`IP`地址，实现这个的`Kubernetes`的抽象就叫`Service`

实现`Service`有多种方式，`Kubernetes`支持`Cluster IP`，上面我们讲过的`Kuber-proxy`的组网，它也支持`nodePort`、`LoadBalancer`等其他的一些访问的能力

![3-4-4](/images/Kubernetes-阿里云公开课学习笔记/3-4-4.png)

#### 3.4.1.5 第五个概念：Namespace

`Namespace`是用来做一个集群内部的逻辑隔离的，它包括鉴权、资源管理等。`Kubernetes`的每个资源，比如刚才讲的`Pod`、`Deployment`、`Service`都属于一个`Namespace`，同一个`Namespace`中的资源需要命名的唯一性，不同的`Namespace`中的资源可以重名

`Namespace`一个用例，比如像在阿里巴巴，我们内部会有很多个`business units`，在每一个 `business units`之间，希望有一个视图上的隔离，并且在鉴权上也不一样，在`cuda`上面也不一样，我们就会用`Namespace`来去给每一个`BU`提供一个他所看到的这么一个看到的隔离的机制

![3-4-5](/images/Kubernetes-阿里云公开课学习笔记/3-4-5.png)

### 3.4.2 Kubernetes的API

下面我们介绍一下`Kubernetes`的`API`的基础知识。从`high-level`上看，`Kubernetes API`是由`HTTP+JSON`组成的：用户访问的方式是`HTTP`，访问的`API`中`content`的内容是`JSON`格式的

`Kubernetes`的`kubectl`也就是`command tool`，`Kubernetes UI`，或者有时候用`curl`，直接与`Kubernetes`进行沟通，都是使用`HTTP + JSON`这种形式

下面有个例子：比如说，对于这个`Pod`类型的资源，它的`HTTP`访问的路径，就是`API`，然后是`apiVesion: V1`, 之后是相应的`Namespaces`，以及`Pods`资源，最终是`Podname`，也就是`Pod`的名字。

![3-4-6](/images/Kubernetes-阿里云公开课学习笔记/3-4-6.png)

如果我们去提交一个`Pod`，或者`get`一个`Pod`的时候，它的`content`内容都是用`JSON`或者是`YAML`表达的。上图中有个`yaml`的例子，在这个`yaml file`中，对`Pod`资源的描述也分为几个部分。

第一个部分，一般来讲会是`API`的`version`。比如在这个例子中是`V1`，它也会描述我在操作哪个资源；比如说我的`kind`如果是`pod`，在`Metadata`中，就写上这个`Pod`的名字；比如说`nginx`，我们也会给它打一些`label`，我们等下会讲到`label`的概念。在`Metadata`中，有时候也会去写`annotation`，也就是对资源的额外的一些用户层次的描述

比较重要的一个部分叫做`Spec`，`Spec`也就是我们希望`Pod `达到的一个预期的状态。比如说它内部需要有哪些`container`被运行；比如说这里面有一个`nginx`的`container`，它的`image`是什么？它暴露的`port`是什么？

当我们从`Kubernetes API`中去获取这个资源的时候，一般来讲在`Spec`下面会有一个项目叫`status`，它表达了这个资源当前的状态；比如说一个`Pod`的状态可能是正在被调度、或者是已经 `running`、或者是已经被`terminates`，就是被执行完毕了

刚刚在`API`之中，我们讲了一个比较有意思的`metadata`叫做“label”，这个`label`可以是一组`KeyValuePair`

比如下图的第一个`Pod`中，`label`就可能是一个`color`等于`red`，即它的颜色是红颜色。当然你也可以加其他`label`，比如说`size: big`就是大小，定义为大的，它可以是一组`label`

这些`label`是可以被`selector`，也就是选择器所查询的。这个能力实际上跟我们的`sql`类型的`select`语句是非常相似的，比如下图中的三个`Pod`资源中，我们就可以进行`select`。`name color`等于`red`，就是它的颜色是红色的，我们也可以看到，只有两个被选中了，因为只有他们的`label`是红色的，另外一个`label`中写的`color`等于`yellow`，也就是它的颜色是黄色，是不会被选中的

通过`label`，`Kubernetes`的`API`层就可以对这些资源进行一个筛选，那这些筛选也是 `Kubernetes`对资源的集合所表达默认的一种方式。

例如说，我们刚刚介绍的`Deployment`，它可能是代表一组的`Pod`，它是一组`Pod`的抽象，一组`Pod`就是通过`label selector`来表达的。当然我们刚才讲到说`service`对应的一组`Pod`，就是一个`service`要对应一个或者多个的`Pod`，来对它们进行统一的访问，这个描述也是通过`label selector`来进行`select`选取的一组`Pod`

所以可以看到`label`是一个非常核心的`Kubernetes API`的概念，我们在接下来的课程中也会着重地去讲解和介绍`label`这个概念，以及如何更好地去使用它

## 3.5 本节总结

1. `Kubernetes`是一个自动化的容器编排平台，它负责应用的部署、应用的弹性以及应用的管理，这些都是基于容器的
1. `Kubernetes`架构是一个比较典型的二层架构和`server-client`架构；

# 4 Kubernetes 网络模型进阶

## 4.1 Service

1. 一组Pod组成一组功能后端
1. 定义一个稳定的虚IP作为访问前端，一般还附赠一个DNS域名，Client无需感知Pod的细节
1. kube-proxy是实现核心，隐藏了大量复杂性，通过apiserver监控Pod/Service的变化，反馈到LB配置中
1. LB的实现机制与目标解耦，可以是个用户态进程，也可以是一堆精心设计的Rules（iptables/ipvs）

__如何实现一个LVS版Service__

```sh
# 第一步，绑定VIP到本地（欺骗内核）
ip route add to local 192.168.166.166/32 dev lo

# 第二步，为这个虚IP创建一个IPVS的virtual server
ipvsadm -A -t 192.168.166.166:16666 -s rr -p 600

# 第三步，为这个IPVS service创建相应的real server
ipvsadm -a -t 192.168.166.166:16666 -r 127.0.0.1:22 -m
```

# 5 参考

* [k8s-阿里云公开课](https://edu.aliyun.com/course/1651?spm=5176.10731542.0.0.785020be217oeD)

