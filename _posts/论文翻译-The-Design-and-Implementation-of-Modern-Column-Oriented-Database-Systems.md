---
title: 论文翻译-The-Design-and-Implementation-of-Modern-Column-Oriented-Database-Systems
date: 2021-08-02 10:15:39
mathjax: true
tags: 
- 原创
categories: 
- Paper
- Database
---

**阅读更多**

<!--more-->

**摘要**

近期，我们对列存储数据库系统（`column-oriented database systems`），或者称为列存储（`column-stores`），做了一个调研，这些系统最显著的特征就是：一个数据表中的不同属性分别存放在不同的文件或存储区域中。伴随着海量数据的扫描、聚合需求的激增，列存储数据库在近几年又重新进入了人们的视野中。列存储的最主要优势在于：仅需要访问、读取查询条件中的那些属性。特别地，我们重点关注三种比较流行的原型，分别是：`MonetDB`[46]、`VectorWise`[18]、`C-Store`[88]。一些著名的商业化列存储系统的实现以这些原型作为理论基石。我们会讨论这三种原型的相似性与差异性，以及针对`compression`、`late materialization`、`join processing`、`vectorization`、`adaptive indexing`这几个方面的架构特点

# 1 Introduction

物理介质（例如磁盘）上的存储效率，以及数据在存储介质与CPU寄存器之间拷贝移动的速率，直接关系到数据库的性能。出于这个原因，数据库社区长期以来都在探索物理存储的不同方案，包括`sophisticated indexing`（复杂索引），`materialized views`（物化视图）以及垂直分区或者水平分区

近几年，列存储重新火了起来。早期有学术影响力的系统包括：`Monet DB`[46]、`VectorWise`[18]、`C-Store`[88]。知名商业化列存储系统包括`Sybase IQ`[66]。`VectorWise`以及`C-Store`分别演化成了商业化系统`Ingres VectorWise`[99]以及`Vertica`[60]。到2013年底，所有主要供应商（包括IBM[11]、微软[63]、SAP[26]以及Oracle）都遵循了这一趋势，并在其数据库系统产品中提供了列存储实现，突出了这项技术的重要性

列存储系统将数据库划分成由独立存储的列值的集合。将每个列独立存储在磁盘上，列存储系统在执行查询任务时，仅需读取所需的列，而不用读取整行（包含不需要的字段，且需要在返回结果前在内存中进行过滤和丢弃）。一个显而易见的优势就是，列存储可以大大提高I/O以及内存的使用效率。在列存储的性能优化这一方向上，可以进行大量的数据库架构方面的创新。在这篇文章中，我们将讨论现代化的列存储系统，包括它们的架构、演进方向以及在数据分析上的优势

![Figure-1-1](/images/论文翻译-The-Design-and-Implementation-of-Modern-Column-Oriented-Database-Systems/Figure-1-1.png)

**数据布局与访问模式**。`Figure 1.1`说明了列存储以及传统行存储（`row oriented databases`/`row stores`）在物理存储上的差异。上图表明，存在三种方式来存储一张包含多个属性的`sales`数据表。在两种列存储的方法中（`Figure 1.1(a)`和`Figure 1.1(b)`），每个列独立地存放在不同的存储单元中。数据通常以数据块的形式在存储介质中进行读写操作，而列存储意味着某个存储单元的每个数据块都包含着`sales`数据表中的某个特定的列值。在这个例子中，如果要查询某个产品在7月的销量，那么仅需访问`prodid`以及`date`这两列，因此只有这两列相关的数据块会被读取（后面会讨论`Figure 1.1(a)`和`Figure 1.1(b)`这两种存储方式的差异）。而对于行存储（`Figure 1.1(c)`），所有的数据都被存放在同一个存储单元中，这就意味着，每个数据块都包含`sales`数据表的所有列。因此在查询时，没有办法仅仅读取所需的列值，而不得不读取整行数据。数据传输效率通常是数据库系统中的主要性能瓶颈，与此同时，数据库表结构的设计也变得越来越复杂，通常一个大表会包含几百个属性，针对这种场景，列存储系统在数据查询中将会表现得异常优异

**权衡**。数据的访问模式决定了如何选择列存储和行存储。假设数据存储在磁盘上。若需要查询单条记录（每条记录都包含了全部的属性值），列存储通常会花费数倍于行存储的时间（每个属性至少得访问一个存储单元，具体数量取决于属性的数量）。若需要查询多条纪录的某几个属性，列存储可以读取大量整列，将搜索分摊到到不同的列。在传统的行存储中，作为对比，若需要查询单条纪录，由于数据是连续存储的，通常只需要一次读取便可读取到所有的列值，并且相比于读取相关列值，读取所有列值带来的整体开销相比于搜索时间来说是微不足道的。然而，当查询的记录不断增大时，读取更多列值带来的整体开销将会逼近甚至超过搜索时间，此时列存储在性能上会比行存储更好。由于这种原因，列存储更适用于数据分析性应用，这些应用通常会查询扫描单表并计算它们的聚合或其他统计信息

**列存储架构**。尽管最近的列存储系统采用了与早期垂直分区研究提案[12, 22, 55, 65]中的概念相似的高级概念，但它们包含了许多超出垂直分区以外的架构特性，旨在最大限度地提高列存储系统的性能。本文的主要目标是调研近些年的研究成果，架构演进趋势，以及优化手段。我们主要关注以下几个方面

* **`Virtual ID`[46]**：在列存储中表示列的最简单方法就是将元组标识符（例如，数字主键）与每一列相关联。显式地存储此元组标识符会使得磁盘数据膨胀，从而降低I/O效率。相反地，现代列存储系统使用元组标识符的偏移量来作为该列的虚拟元组标识符，从而避免额外的存储（`Figure 1.1(a)`和`Figure 1.1(b)`）。在一些列存储系统中，每个属性都存储为固定宽度的密集数组，并且每个记录都存储在表的所有列中的相同（数组）位置。另外，宽度固定的列仅需要通过偏移量就能实现数据访问，极大地简化了数据访问的过程。举个例子，若要访问列`A`的第{% raw %}$i${% endraw %}个列值，只需要访问起始位置为{% raw %}$startOf(A) + i * width(A)${% endraw %}的数据，不需要其他的间接引用。然而，正如我们稍后将在第4节中详细讨论的那样，相比于行存储而言，列存储的一个优势就是提高了压缩率，许多压缩算法以非固定长度的方式压缩数据，因此数据不能简单地存储在数组中。一些列存储系统会放弃部分压缩性能从而保持固定宽度的特性，另一些列存储系统仍然利用非固定宽度的压缩算法
* **`Block-oriented and vectorized processing`[18, 2]**：通过在算子间传递缓存快大小的元组块（`cache-line sized blocks of tuples`），算子同时并发操作多个值而非使用传统的元组迭代器，列存储可以显著提高缓存利用率和CPU使用效率。使用矢量化CPU指令对这些值块进行选择、表达式和其他类型的算术运算可以进一步提高吞吐量
* **`Late materialization`[3, 50]**：`Late materialization`或者`late tuple reconstruction`（把从各个列中获取的数据重新组装为行的过程称之为`tuple construction`）指的是延迟元组的聚合。事实上，针对某些查询，列存储可以完全避免这种聚合行为。因此，`Late materialization`意味着列存储不仅一次存储一列数据，而且还以列格式处理数据。举个例子，选择算子使用for循环，一次扫描一列，并以缓存/CPU友好的格式输出（与首先构造包含当前查询所需的所有属性的元组，并将它们提供给传统的行存储选择算子相反，该算子只需要访问这些属性中的一个）。通过这种方式，列存储能极大的提升内存利用率（避免浪费内存资源）
* **`Column-specific compression`[100, 2]**：通过对每一列使用针对该列最有效的压缩算法，能够大幅度地降低整体的磁盘使用量。由于列存储的特性，即便利用简单的压缩算法也能得到较好的压缩率
* **`Direct operation on compressed data`[3]**：许多现代的列存储系统仅在必要时对数据进行解压缩，理想情况下，直到结果需要呈现给用户时才进行解压缩。处理压缩数据能够极大地提升内存利用率，这也是主要的瓶颈。`Late materialization`允许数据以压缩的形式存储在内存中，而创建更宽的元组通常需要先进行解压缩
* **`Efficient join implementations`[67, 2]**：由于列值独立存储，类似于半聚合（`semi-join`[13]）的聚合策略也是可能的。对于某些特定的聚合类型，其性能远远优于传统的`hash join`或`merge join`
* **`Redundant representation of individual columns in different sort orders`[88]**：根据特定属性排序的列可以在该属性上更快地过滤。通过存储按不同属性排列的多个副本，可以大幅度地提高性能。`C-Store`将按照某个特定属性排序的不同列称为投影。`Virtual ID`是每列基于自身的投影。此外，列值有序存储还能进一步提高压缩率
* **`Database cracking and adaptive indexing`[44]**：`Database cracking`避免排序整个列。具有`cracking`特性的列存储可以自适应地以及增量地对列进行排序（排序这一结果是作为查询的副作用），`workloads`无额外的开销。每个查询操作都可能会重组对应的列，从而加速后续查询的速度。定宽列存储能够加速列重组的速率，同时矢量处理意味着我们可以一次性有效地重组整个列，从而使得自适应索引成为现代列存储架构的重要特性
* **`Efficient loading architectures`[41, 88]**：最后，列存储的一个问题是它们的加载和更新速度可能比行存储慢，这是因为列存储中，每个列是单独存储，一行数据的写入要分别写入不同的列存储单元中，以及数据是以压缩的形式存储。由于加载性能可能是数据仓库系统中的一个重要问题，因此优化的加载器很重要。例如，在`C-Store`系统中，数据首先被写入一个未压缩的，写优化的缓冲区中（`WOS`），后续再进行批量的压缩和写入磁盘。这种方法避免了对每个属性、每行进行一次磁盘搜索，并且不必将新数据插入压缩列中；而是一次写入和压缩许多记录

**上述特性是列存储所特有的吗？**上面讨论的这些特性以及概念，稍加改变便也应用到行存储系统当中。事实上，大部分设计特征都受到了早期针对行存储研究的启发。多年来，学术界和工业界在传统的行存储领域，通过一些附加设计达到了相似的效果。这些附加设计旨在不干扰行存储的基础架构

举个例子，`IMB DB2`中的`EVI`功能早在1997年就允许数据以列的方式进行存储[14]。相似地，过去针对`fractured mirrors`[78]的研究提出系统存储数据的两个副本，一个副本以行的形式存储，另一个副本以列的形式存储，或者更复杂的存储形式，例如`PAX`[5]，每个关系元组都像普通行存储一样存储在单个页中，但是每个页内部都按列进行组织，这种方式对磁盘I/O并无益处，但是会降低数据在磁盘、主存、和CPU寄存器之间的传输。传统数据库的索引技术也与`Late materialization`有异曲同工之妙，它允许算子在某一个时刻仅处理部分相关的数据，能够更好地优化内存使用。现代索引指导工具[21]，总是尝试提出一套“覆盖”索引，它指的是一组索引，在理想情况下，任何查询操作都可以使用到这些索引，从而避免直接访问数据（行存储）。早期的系统，例如`Model 204`[72]严重依赖位图索引[71]来降低I/O以及处理成本。与矢量化（`vectorization`）类似的概念最早出现在行存储中。另外，压缩技术也是最早出现在行存储中[30, 82]，同时还研究了一些设计原则，例如尽可能晚地解压缩数据[30]以及同时压缩数据以及索引[31, 47]等

本文对于列存储的贡献是（除了提出新的数据存储和访问技术）：一种全新的为上述分析型应用设计的架构。从零开始，我们可以毫无顾忌地将这些理念、想法推向极致，而无需担心与传统设计的兼容性。在过去几年中，这些设计理念的一些变体各自被独立地试验过，主要集中在传统行存储的原型设计。相比之下，从数据库存储开始，沿着技术栈往上，包括查询执行引擎和查询优化器，列存储系统从设计上与传统的行存储系统存在巨大差异，因此在数据库的各个方面，能够在创新的同时最大限度地发挥这些想法的好处。我们会在4.9节重新讨论行存储与列存储

![Figure-1-2](/images/论文翻译-The-Design-and-Implementation-of-Modern-Column-Oriented-Database-Systems/Figure-1-2.png)

为了说明列存储和这些优化的好处，我们简要总结了最近一篇论文[1]的结果。在这篇文章中，我们比较了学术原型`C-Store`以及一个商业化行存储系统。我们研究了各种列存储优化对`SSBM`[73]（`TPC-H`数据仓库基准测试的简化版本）的整体查询性能的影响。`Figure 1.2`显示了基准测试中规模为10的数据库（6000万个元组）中所有查询的平均运行时间。左边的条形图显示了`C-Store`中各种优化手段的性能增益。`baseline`代表包含所有优化项，需要花费4s来响应所有的查询，而不含所有优化项的需要花费40s来响应所有的查询。右边的条形图显示了传统的商业化行存储系统。基于这些结果，可以发现经过充分优化的列存储系统的性能是传统商业化行存储系统性能的5倍。但是未经过优化的列存储系统的性能要落后于行存储系统，其表现较差的可能原因是`SSBM`基准测试使用了相对较小（列少）的表格，因此列存储所带来的的增益也会被削减。在大多数的现实系统中，列的数量会比较大，因此，列存储带来的性能增益会更加显著

尽管比较成熟的商业系统和学术原型之间的绝对性能数字很棘手，但这些数字表明，未优化的列存储与选择大部分列的查询提供了与行存储系统相当的性能，并且前面提到的优化项可以带来非常大的性能提升

在本文的剩余篇幅中，我们会展示上述架构创建如何带来巨大的性能提升。特别地，我们会详细的讨论`C-Store`、`MonetDB`、`VectorWise`，介绍他们的异同点，以及三种架构的核心创新点

在下一节中，我们追踪了数据库文献中垂直分区和列存储系统的演变，讨论面向列的架构更适用于分析型场景这一技术趋势。在第3节和第4节中，我们将介绍`C-Store`、`MonetDB`、`VectorWise`的顶层架构以及详细设计，以及一些商业化实现。最后在第5节，我们讨论未来的发展趋势以及进行总结

# 2 History, trends, and performance tradeoffs

尽管列存储技术在1970年就已问世，但是直至2003年，列存储相关的研究才被大家认可，商业化列存储才开始出现。在本小节中，我们将追踪列存储的发展历史，技术以及应用的发展趋势。最后总结了最近关于列存储以及行存储之间基本性能如何权衡的研究结果

## 2.1 History

列存储技术的起源可以追溯到1970年代，那时转置文件（`transposed files`）刚刚问世[65, 12]。TOD（`Time Oriented Database`）是一种基于转置文件，专为病例管理而设计的系统[90]。最早出现的与现代列存储系统类似的系统叫做`Cantor`[55, 54]。它具备整数压缩技术，包括`zero suppression`、`delta encoding`、`RLE (run length encoding)`以及`delta RLE`，所有这些技术都应用在现代的列存储系统当中（我们会在后续小节中详细讨论这些技术）。使用动态规划算法选择压缩方法和相关参数

![Figure-2-1](/images/论文翻译-The-Design-and-Implementation-of-Modern-Column-Oriented-Database-Systems/Figure-2-1.png)

紧跟着转置文件的研究热潮，又兴起了针对表属性类聚的垂直分区技术的研究。于此同时，行存储技术成为了关系型数据库的架构标准。页内存储的典型实现是`slotted-page approach`，如`Figure 2.1`左半部分所示。这种存储模型被称为是`N-ary Storage Model（NSM）`。1985年，`Copeland`、`Khoshafian`共同提出了`NSM`的替代方案`Decomposition Storage Model（DSM）`，这是列存储的前身[22]，如`Figure 2.2`右半部分所示。对于许多人来说，这项工作标志着行存储和列存储的第一次全面比较。在随后的20年间，术语`NSM`以及`DSM`更常用于替代行存储和列存储这两个术语。在`DSM`中，表的每一列都单独存储，并且对于列中的每个属性值，存储`surrogate key`（类似于记录id）的副本，如`Figure 1.1(b)`。由于`surrogate key`冗余存储，因此相比于`NSM`需要更多的存储空间。以与原始表相同的顺序存储每一列（在`surrogate key`上使用聚集索引），作者提议为每列的属性存储一个非聚集索引，从而提供可以将任何属性快速映射到`surrogate key`的能力

一项分析（基于当时可用的技术）表明，当仅查询几列时，`DSM`的扫描性能优于`NSM`，但是以额外的存储空间为代价。当查询的列的数量增多时，`DSM`的性能逐渐下降，作者重点介绍了`DSM`简单性和灵活性方面的优势。他们推测基于`DSM`的存储的物理设计决策会更简单（因为不需要做索引创建的决策），并且更容易构建`DSM`的查询执行引擎。最早的`DSM`论文并没有涉及压缩算法，也没有评估除了扫描算子外的其他算子的优势。后续论文侧重于研究`DSM`架构之上的算子的并行性[59]，而随后对聚合和投影索引的研究[58]则进一步加强了`DSM`相对于`NSM`的优势

尽管上述研究尽可能地围绕着`DSM`指出了列存储相比于行存储的优势，直到很久以后，大约在2000年左右，技术和应用层面的发展趋势才为列存储技术的发展铺平了道路

## 2.2 Technology and Application Trends

就其核心而言，关系数据库管理系统的基本设计至今仍与1980年代开发的系统非常接近[24]。然而硬件确发生了翻天覆地的变化。在1980年，`Digital VAX 11/780`具有`1 MIPS`CPU、`1KB`高速缓存、`8 MB`最大主内存、`1.2 MB/s`传输速率和`80MB`容量的磁盘驱动器，标价`25`万美元。而到了2010年，服务器的CPU通常快`5000`到`10000`倍，更大的缓存和`RAM`大小，以及更大的磁盘容量。硬盘驱动器的磁盘传输时间提高了大约`100`倍，平均磁盘磁头寻道时间提高了`10`倍（`30`毫秒与`3`毫秒）。这些趋势的差异（`10000`倍与`100`倍与`10`倍）对数据库的性能产生了重大影响

磁盘容量增长与磁盘传输和磁盘寻道次数的性能提升之间的不平衡可以通过两个指标来查看：a) 每个可用字节的传输带宽（假设整块磁盘都被占用），多年依赖已经降低了2个数量级；b) 顺序存取速度与随机存取速度之比，多年依赖已经提高了1个数量级。这两个指标清晰地表明，`DBMSs`不仅需要尽可能避免随机磁盘`I/O`，而且最重要的是要保留磁盘带宽

随着整个内存层次结构的随机访问变得越来越昂贵，查询处理技术开始越来越依赖于顺序访问模式，大多数`DBMS`架构都是围绕着应该尽可能进行完全顺序访问的前提而构建的。然而，随着数据库的体积增大，扫描变得越来越慢。大多数数据库供应商并不认为`DSM`是`NSM`的可行替代品，这是由于早期`DSM`实现中发现的局限性[22]，其中`DSM`仅在查询访问很少的列时优于`NSM`。为了让基于列 (`DSM`) 的存储方案优于基于行 (`NSM`) 的存储方案，它需要有一种快速重建元组的机制（因为`DBMS`的其余部分仍将在行上运行），并且还需要能够在访问磁盘上的多个列时分摊磁盘搜索的成本。高性能CPU能够实现前者，而大容量的内存能够实现后者

尽管现代列存储因能够高效地处理基于磁盘的数据而广受欢迎，在1990年代，列存储技术广泛用于内存系统。在90年代末期，人们对研究内存数据布局以解决CPU和内存之间日益增长的速度差异产生了浓厚的兴趣。大约在1980年代，内存访问与指令执行的速度相当。但是到了90年代中期，内存访问的周期是指令执行周期的数百倍。`MonetDB`[46]是学术界第一个主要的列存储项目。`MonetDB`最初的动机，是为了解决内存带宽问题，同时通过避免表达式解析提高计算效率[19]。一种新的查询执行代数是在类似于具有`Virtual ID`的`DSM`的存储格式上开发的。随后的研究，研究了缓存敏感查询处理算法（`MonetDB`的全面介绍在第3.2 节之后）

PAX（`Partition Attributes Across`）采用混合`NSM/DSM`方法，其中每个`NSM`页面组织为一组迷你列[5]。它延续了`NSM`的`I/O`模式的同时优化了缓存和RAM之间的通信（`seeking to obtain the cache latency benefits identified in Monet without the disk-storage overheads of DSM with its explicit row IDs`）。后续的项目包括`data morphing`[39]（`PAX`的动态版本）、`Clotho`[84]，研究了采用`scatter-gather I/O`的自定义页面

`Fractured Mirrors`[78]利用镜像来提高可靠性和可用性。它的思路是，一份数据以`NSM`的格式存储，另一份数据以`DSM`的格式存储，因此，能够同时得到两者的优势

大约在1996年左右，第一个商业化的列存储系统`SybaseIQ`[28, 29, 66]问世，展示了压缩的，面向列的存储能够为多种分析型应用带来好处。尽管它在商业上获得了成功，但是它未能吸引其他数据库供应商或者学术界的关注，可能原因包括，由于它进入市场为时过早，有利于列存储的硬件进步（触发数据库架构创新），例如大容量内存、`SIMD instructions`，在当时并未问世；也可能是因为它缺乏一些架构创新，后来证明这些创新对于列存储的性能优势至关重要

行业中一些值得注意的例子包括`IBM BLU`[79]，它起源于`IBM Blink`项目[11]，主要在提供与处理压缩列的能力紧密集成的架构上进行创新，以及存储数据的`SAP HANA`[26]，它以行格式和列格式将在线分析处理和在线事务处理结合在一个系统中。此外，微软很快就对`SQL Server`行存储的架构进行了扩展，带来了面向列的存储、矢量化处理和压缩等功能[62, 61]。最初列存储仅用于辅助加速器架构，例如列索引[62]，但是后续的版本提供了更通用的架构，它允许以列的形式存储基础数据

## 2.3 Fundamental Performance Tradeoffs

尽管`DSM`使得快速扫描单列成为可能，但是扩展到扫描多个列，甚至扫描全表，其性能远远落后于`NSM`。这是由于多列的元组聚合、更多的磁盘访问、处理无关信息等会带来大量的性能开销。为了使得列存储能够获得与行存储相当的性能，需要在`DSM`的各个工作负载上都保持良好的性能，包括查询多列甚至整行的这种情况。随着CPU处理速度的增长远远大于磁盘带宽的增长，以及在软件层面针对I/O的优化，使得列存储在读取多列时的性能慢慢接近行存储。这一点，在过去十年中，被许多学术研究所证明

* 在`Fractured Mirrors`[78]中，作者针对`DSM`提出了多个优化项。每个`DSM`列以`B-tree`的方式存储，每个叶节点包含了列的所有属性值。此外，淘汰了每列的`ID`属性，将开销分摊到多个不同的列中（`Graefe`还提出了一种在`B-tree`中高效存储列值的方法[33]），以及使用了基于块的元组重建
* 在文献[40]中，作者基于一种全新的实现（一个独立的存储管理器，读优化的存储方式，大型预选单元来隐藏跨列的磁盘查找），对比了列扫描器和行扫描器。在2006年，在查询整行数据的情况下，列存储扫描的速率仅比行存储慢20-30%
* 最后，在文献[89]中，作者提出了闪存固态存储器是数据库系统的主要介质，并证明了基于列的存储模型对磁盘存储的有效性。由于`SSD`的随机访问速度远优于`HDD`，列存储在这种介质上，进行整行全表扫描的I/O开销与行存储相当

![Figure-2-2](/images/论文翻译-The-Design-and-Implementation-of-Modern-Column-Oriented-Database-Systems/Figure-2-2.png)

`Figure 2.2`将上述研究成果都整合到一张图中。展示了列存储（或称`DSM`）的扫描时间与行存储的扫描时间（`I/O`开销是个常量）在不同投影比例（读取元组/列的百分比）下的对比。`DSM`的基线取自于2001年的参考文献[5]，该基线同样被`PAX`以及`NSM`的性能对比所引用。随着时间的推移，在最差的情况下（投影比例是100%），列存储的性能开销在慢慢逼近行存储的性能开销。当存储介质是`SSD`时，列存储的性能不再若于行存储，当仅查询少数列时，列存储的性能要远远优于行存储。如果选择性很高，那么列存储可以最大限度地减少它们创建的中间结果的数量，否则会产生显着的开销

# 3 Column-store Architectures

在本小节中，我们将讨论`C-Store`、`MonetDB`以及`Vector-`Wise三种研究原型的顶层架构。这些架构衍生出了现代列存储系统的主要设计原则。本文高度赞扬这些系统的设计，大部分设计原则三种系统都遵循，少部分设计原则是各自系统所独有的。下一节更详细地讨论这些特性和设计原则，并提供查询处理和性能示例

## 3.1 C-Store

在`C-Store`中，数据在磁盘中的主要组织形式是一组列文件。每个列文件包含了某列的所有数据，这些数据是通过一些列压缩算法压缩过的，并且以与列相关联的一些属性排序过后的。这一组文件被称为`ROS（read-optimized store）`。此外，新数据是通过`WOS（write-optimized store）`方式存储（数据未压缩，且未分区）。`WOS`支持高效地加载数据，并且能够分摊压缩和搜索的时间开销。一个被称为`tuple mover`的进程会定期的将`WOS`的数据转换成`ROS`的数据，执行的操作包括排序、压缩、将数据重新写入等等操作

![Figure-3-1](/images/论文翻译-The-Design-and-Implementation-of-Modern-Column-Oriented-Database-Systems/Figure-3-1.png)

`C-Store`中的每列，可能会以不同的排序方式存储多次。一组以相同排序方式排序的列，被称为投影（`projections`）。通常来说，至少包含一个投影（包含所有列）用以响应所有的查询请求。包含少量列，且按照不同排序方式的投影，主要是为了优化热点数据的查询性能。举个例子，在特定时间段内，特定区域每个月的销售数量的查询，可以受益于包含产品id、日期、地域且以地域、时间进行排序的投影。排序能够对相关的记录进行有效的子集化，且可以一次聚集一个月的结果，而无需维护聚合的中间状态。作为对照，在特定时间段内，每个月的销售数量的查询，则可以受益于包含日期且以日期进行排序的投影。`Figure 3.1`展示了两种不同的投影。在`C-Store`中，我们使用`saleid, date, region | date`来表示包含`saleid`、`date`、`region`这三列，且以`date`进行排序的投影。注意到，这些投影可以包含不同的列，且无需包含所有的列。

`C-Store`中的每列都是经过压缩处理的，且不同数据可能使用不同的压缩算法。如何选择压缩算法，取决于：a) 数据是否排序；b) 数据类型；c) 该列不同取值的数量。举个例子，`product class`的可能取值就很少，当数据经过排序后，使用`using run-length encoding（RLE）`便可对其进行有效压缩。在`RLE`中，连续`X`个相同的数值，可以被表示为`(X, product class)`，而不是`X`个记录。更多压缩算法的细节将在第4节中进行讨论

`C-Store`不支持二级索引，但是通过使用稀疏索引（`sparse indexes`）支持对排序投影的有效索引。稀疏索引是一种基于树的小型索引，用于存储列的每个物理页上的第一个数据。通常来说，`C-Store`的每个物理页的大小在几兆字节。给定投影中的某个数值，查询会返回第一个包含该值的物理页。然后在物理页中扫描来查询该值。`C-Store`中另一种相似的稀疏索引以元组的下标来进行排序，当数据以压缩的形式存储，或者存储的是可变长度的属性值时，允许通过元组偏移量来快速地查询某个元组信息

此外，`C-Store`使用无覆盖（`no-overwrite`）的存储形式。在这种模式下，更新操作就会被等价表示为先删除再插入，删除操作就会等价表示为在删除列（`delete column`）中增加一条记录

`C-Store`中的查询执行过程，同时包含访问`ROS`以及`WOS`数据，并整合这两者的结果。在特定时间范围内的查询，需要过滤掉存在于删除列中的数据。因此，`C-Store`允许查询过去某个时间段的数据（由于数据并未删除，而是记录在删除列中，那么就可以通过比较删除的时间点与查询的时间点来判断数据是否可见）。修改数据库的查询使用传统的两阶段锁定运行。如果只读查询可以容忍读取稍微过时的数据，则可以通过在最近的某个时间执行它们而无需设置锁来运行它们。最后，`C-Store`查询执行器使用了一系列先进的技术，包括物化技术、列聚合技术，批处理技术。这些技术将在第4节中详细讨论

最后，除了完整的垂直分区之外，`C-Store`被认为是一个无共享的大规模并行分布式数据库系统，尽管学术原型从未包含这些功能（商业版本则包含）。`C-Store`中的并发设计指的是投影以`hash`或者`range-partitioning`的方式散列在不同的节点上，查询尽可能地在每个节点上执行，然后聚合成一个结果并返回。大多数`C-Store`的并行设计都是基于早期的共享非并行系统的设计，例如`Gamma`[23]。因此，并行不是这里讨论的重点

## 3.2 MonetDB and VectorWise

在本小节，我们先讨论`MonetDB`的架构设计，然后再讨论`VectorWise`的架构设计

**`MonetDB`**。`MonetDB`是从头开始设计的，专注于在现代硬件上执行高效的分析任务。`MonetDB`同时将数据存放在内存以及磁盘上，并且使用了批处理以及物化技术。它完全依赖于内存映射文件，避免了管理缓冲池的开销和复杂性。`MonetDB`与传统的`RDBMS`在许多方面上存在差异，包括

* 执行引擎，使用`a column at-a-time-algebra`[19]
* 处理算法，旨在最大限度地提高缓存命中率而非`I/O`效率
* 索引，这不是`DBA`的任务，而是作为查询执行的副产品（写查询的人得明白如何利用索引高效地查询数据），即`database cracking`
* 查询优化，这是在运行时完成的，在查询的过程中会对数据进行一些处理，包括排序等等
* 事务管理，使用显式的附加表以及代数运算实现，因此只读的工作负载可以省略并避免所有事务开销

传统的查询过程采用的是每次一个元组（`tuple-at-a-time`）、拉模式（`pull-based`）的方式，每个算子通过调用`next()`方法从上下游中的相关算子中获取下一个输入元组，并以此方式进行迭代处理。而`MonetDB`中的每个算子工作在列上。通过这种方式，`MonetDB`旨在模仿计算机在提升CPU执行效率方面取得的成功，通过在列上的处理模式优化（例如`tight loops over fixed-width and dense arrays`）获取性能增益。这些优化需要编译技术的支持，通过各种手段从CPU中获取最大的性能，这些手段包括`strength reduction`（将一个算子替换为另一个低成本的算子）、`array blocking`（对数组的子集进行分组以增加缓存局部性）以及`loop pipelining`（将循环映射为流水线的执行方式）。`MonetDB`的`column-at-a-time`这一原语，不仅可以使用更少的指令来完成同样的任务，而且由于消除了`tuple-at-a-time`，指令执行的效率会更高。换言之，就是`MonetDB`查询计划为CPU提供了更多的动态指令，使得流水线满负荷运行、可以进行分支预测以及提高CPU缓存命中率。并且在编译器的帮助下允许数据库系统从`SIMD`中获得指示

`column-at-a-time`处理过程，通过`BAT Algebra`来实现，允许算子处理一部分`BAT`并产生新的`BAT`。`BAT`代表二进制关联表`Binary Association Table`，指的是`DSM`中的`<surrogate, value>`表。其中`surrogate`就是`Virtual ID`，它实际上是列的数组索引，并没有具体化。基础数据以及中间结果都会存储在`BAT`中，最终结果其实是一组`BAT`。因此`MonetDB`将`late tuple materialization`发挥到了极致。`BAT`本质上是内存中的（或者内存映射）数组。算子产生或者消费`BAT`，例如选择算子将一个`BAT`作为输入，对该`BAT`中的数据进行筛选过滤，然后产生一个新的包含所有合法元组的`BAT`作为输出

![Figure-3-2](/images/论文翻译-The-Design-and-Implementation-of-Modern-Column-Oriented-Database-Systems/Figure-3-2.png)

元组重建的缺失正符合`MonetDB`的另一个目标，即用一个内部的数据表示形式（即`BAT`）来处理其他不同模式的数据。`MonetDB`遵循了`frontend/back-end`的设计，其中`front-end`负责维护存储在一些逻辑数据模型中的数据的视图；`MonetDB`中的`front-end`适用于存储和查询纯关系型数据，包括面向对象的，`XML RDF`以及图数据。`front-end`将终端用户输入的查询语句（`SQL`、`OQL`、`XQuery`、`SPARQL`）转换成`BAT`代数，执行计划，并将结果`BAT`转换成最终的结果。`Figure 3.2`展示了不同的`front-end`产生的查询计划都会在`back-end`中进行执行

`BAT`代数效率背后的原因是它的硬编码语义，导致所有算子都是无谓词的。作为对照，在传统数据库系统的关系代数中，聚合以及选择算子采用布尔表达式来确定哪些元组应该被聚合或选。由于这些布尔表达式，在查询时才能确定，意味着`RDBMS`必须在选择和聚合算子的执行过程中使用一些表达式解释器来对表达式进行解释。这样的谓词不会出现在`BAT`代数中。因此我们也称其为「零自由度」。零自由度意味着算子将不再需要表达式解释器。因此，所有的`BAT`代数中的算子的执行代码都是固化的。从而，查询中不同的查询条件将会被映射成不同的算子。`MonetDB`中表达式解析的粒度发生在`column-at-a-time`这个维度，因此分摊了解析带来的开销

`BAT`代数背后的理念也可以解释为：`RISC`方法在数据库查询语言上的引用。通过简化代数，提供了优化执行效率的可能性

近期的研究表明，通过走极端路线并即时编译代码（即在查询处理期间），可以获得进一步的优势。理由是编译后的代码最适合特定查询的查询模式和数据布局，从而提高扫描密集型工作负载的性能[43, 70]，使用针对这次特定扫描所关联的算子，可以进一步减少函数调用，增加缓存命中率

在执行数据更新时，`MonetDB`为数据库中的每个基本列都使用了一组`pending column`。每个更新操作仅影响`pending column`，也就是说每个更新操作，会转换成针对`pending column`的操作。每个查询都会从基础列以及`pending column`中读取数据，并聚合成最终的值。举个例子，当针对`X`列进行一次过滤动作时，会执行两个算子，其中一个算子针对`X`列进行过滤；而另一个算子对该列对应的`pending column`组进行过滤，随后对两个算子的结果进行合并，`pending column`中的数据会定期的与基础列中的数据进行合并

**VectorWise**。`MonetDB`开创了多个列存储的核心设计原则，后续的`VectorWise`以及`C-Store`又补充了一些标志性的设计。`MonetDB`将数据以非压缩的方式存储在磁盘上，`BAT`代数算子通过内存映射来访问磁盘，不受任何API的阻碍。由于缺少`buffer`，`MonetDB`依赖操作系统的虚拟内存访问，系统本身对`I/O`调度没有绝对控制权。`column-at-a-time`的另一个缺点是它对中间结果的全物化。举个例子，如果选择算子将全部数据作为一次输入，那么该算子需要一次性物化所有的结果，这会导致额外的开销，特别是处理海量数据的时候。上述缺点综合在一起，就导致了，当`MonetDB`工作空间（内存等）超过`RAM`时，就极易发生`swap`（该操作性能极差）

上述问题被同一个位于`CWI`的研究小组开发的系统`VectorWise`[96]所攻克。`VectorWise`另起炉灶，重新实现一整套系统，以解决`MonetDB`的缺陷，并提供为现代化硬件量身定制的架构。`VectorWise`的核心创新点它的矢量执行模型。在`MonetDB`（对中间结果的全物化）以及传统的行存储系统（`tuple-at-a-time`迭代模式带来的函数开销）之间找到了平衡点。本质上来说，`VectorWise`一次处理某列的一部分，而不是一次处理整列，或一次处理整行

`VectorWise`以更加高级的方式执行显式`I/O`操作，自适应地为并发查询在`Active Buffer Manager（ABM）`以及`Cooperative Scans`[98]等方面进行优化。`VectorWise`提供了一种新颖的更新数据的方法（`Positional Delta Trees`[41]），以及一种高效率的压缩算法[100]。我们将在下一小节讨论这些细节

## 3.3 Other Implementations

来自行业的后续设计，基本共享了`VectorWise`以及`C-Store`的基本原则（我们将在下一节中详细讨论）。这里有2个在列存储领域，广泛被工业界采纳的核心架构

**Columnar Storage Only**。第一范式：以列的方式存储数据，但依赖于标准的行存储执行引擎来处理查询。这意味着，每个查询仅访问相关的列，在`I/O`层面节省了一些开销，但是一旦所有数据到达内存，它就会立刻拼接成`N`元组并送入经典的行存储引擎。因此，这种设计容易被现有系统所兼容，因为只需要在列存储中读出数据之后，将其映射成元组即可，但是这种方式不能发挥列存储的全部优势。此类设计的典型实现包括`Teradata`、`Asterdata and EMC`、`Greenplum`。这种设计的好处就是它允许更平滑的过度到一个全新的架构中去，并且他允许数据同时以行存储以及列存储的方式存储，且仍然可以由同一个执行引擎处理这两种数据

**Native Column-store Designs**。第二范式：全方位地接纳列存储这一设计理念。它不仅以列的方式存储数据，而且为`column-at-a-time`的算子以及延迟物化等技术量身打造了一个执行引擎。然后，这个新的引擎将与传统的行存储引擎集成到一起

**IBM BLU/BLINK**。第二范式的一个主要例子是源自`IBM BLINK`项目[11, 52]的`IBM BLU`[79]。本质上，`IBM BLU`位于标准行存储`DB2`引擎的一侧，负责处理部分数据。优化器知道何时将数据输入到传统的标准引擎，以及何时将数据输入到`BLU`引擎。通过这种方式，查询可以从列存储中获益，反之亦然，实际上查询可以从面向行和面向列的表中扫描数据

除了利用标准的列存储设计，例如延迟物化。`IBM BLINK/BLU`在压缩领域引入了新的技术。`Frequency partitioning`[81]在提升压缩的空间效率的同时遵循了列存储的设计原则。总体思路是重新组织列，以减少数据在每个数据页中的变化度。也就是说，通过字典压缩，并通过最小化页面内的可能值来分别压缩每个页面。`IBM BLINK/BLU`降低了数据表示的成本（每个数据以更少的bit来存储）

`Frequency partitioning`意味着，与其他系统不同，`IBM BLU`可以存储非定宽的列。每个页面有独立的字典以及编码长度；在每个页面中，所有的数值/编码都是定长的，但是同一列的不同页面可能采用不同长度的编码。因此，与其他列存储系统相似，`IBM BLU`可以利用依赖于紧密`for`循环并且对缓存和`CPU`友好的算子的设计。只不过在处理不同的页面时，需要进行一些微调。这会导致页面的设计更加复杂一些，它不是纯粹的基于数组的存储，而是需要存储一些额外的信息，包括每个页面独立的字典以及元数据信息（元组与全局顺序的映射关系）。鉴于`Frequency partitioning`重新组织数据且通常发生在单个列的维度，这意味着，不同的列可能以不同的顺序存储，因此需要有一种方法能够将一张表中的所有列关联起来。我们将在下一节中讨论`Frequency partitioning`以及其他核心的设计原则

**Microsoft SQL Server Column Indexes**。这个设计范式，被由`Microsoft`[62]研发的`SQL Server`系统所采用。`SQL Server`对列存储和列执行提供了原生的支持，同时采用了许多列存储中的重要设计，例如矢量处理，并且大量利用压缩。`SQL Server`将这些设计与传统的行存储系统进行了有效的整合，允许根据使用场景灵活地选择使用行存储还是列存储。列可以用作“列索引”，即辅助数据，增强对特定属性的扫描性能，或者它们可以作为扫描密集型场景的首选

# 4 参考

* [The Design and Implementation of Modern Column-Oriented Database Systems.pdf](/Papers/The-Design-and-Implementation-of-Modern-Column-Oriented-Database-Systems.pdf)

# 5 todo

1. OLAP、OLTP
1. Database cracking的定义
1. EVI
1. transposed files
1. 列存储和行存储是数据库系统最底层的部分，这部分的差异，顺着技术栈往上走，如何影响上层模块的架构？
1. DBMS、RDBMS
1. Disk Bandwidth的定义
1. 在数据库的上下文中，load指的是存储数据还是读取数据？
1. WOS支持高效地加载数据，并且能够分摊压缩和搜索的时间开销？为什么
1. 什么是BAT？
1. 算子的谓词是指什么？
1. RISC
